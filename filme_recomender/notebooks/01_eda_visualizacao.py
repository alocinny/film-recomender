import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import sys
import os

# Adiciona a pasta raiz ao path para conseguir importar o src
sys.path.append('..') 
from src import config

# Configura√ß√µes Visuais
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 12
PALETTE = "mako"

print("‚úÖ Bibliotecas importadas e ambiente configurado.")

# 1. Carregar Dados Brutos (Para an√°lises iniciais)
ratings = pd.read_csv(config.DATA_RAW / 'ratings.csv')
movies = pd.read_csv(config.DATA_RAW / 'movies.csv')
tags = pd.read_csv(config.DATA_RAW / 'tags.csv')

print(f"Dados Brutos Carregados:")
print(f"Ratings: {ratings.shape}")
print(f"Movies: {movies.shape}")

# 2. Carregar Dados Processados (Para analisar correla√ß√µes e features criadas)
# S√≥ vai funcionar se voc√™ j√° rodou o 'main.py' ou 'data_prep.py'
try:
    df_enriched = pd.read_parquet(config.DATA_PROCESSED / 'ratings_enriched.parquet')
    print(f"Dados Enriquecidos Carregados: {df_enriched.shape}")
except FileNotFoundError:
    print("‚ö†Ô∏è Aviso: 'ratings_enriched.parquet' n√£o encontrado. Rode o pipeline de dados primeiro para ver correla√ß√µes.")
    df_enriched = None

# C√°lculo de Sparsity
n_users = ratings['userId'].nunique()
n_items = ratings['movieId'].nunique()
n_ratings = len(ratings)
total_possible = n_users * n_items
sparsity = 1 - (n_ratings / total_possible)

print(f"Sparsity do Dataset: {sparsity:.4%}")

# Gr√°ficos
fig, axes = plt.subplots(1, 2, figsize=(18, 6))

# 1. Distribui√ß√£o Global
sns.countplot(ax=axes[0], x='rating', data=ratings, palette=PALETTE)
axes[0].set_title('Distribui√ß√£o Global dos Ratings')
axes[0].set_xlabel('Nota')

# 2. Heatmap de Esparsidade (Zoom Top 100)
user_counts = ratings['userId'].value_counts()
top_100_users = user_counts.head(100).index
movie_counts = ratings['movieId'].value_counts()
top_100_movies = movie_counts.head(100).index

sample_df = ratings[
    ratings['userId'].isin(top_100_users) & 
    ratings['movieId'].isin(top_100_movies)
]
sample_matrix = sample_df.pivot(index='userId', columns='movieId', values='rating')

sns.heatmap(sample_matrix.notna(), cmap='Blues', cbar=False, xticklabels=False, yticklabels=False, ax=axes[1])
axes[1].set_title('Zoom de Esparsidade (Top 100 Users x Items)')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# 1. Ratings por Filme
movie_counts = ratings.groupby('movieId').size().sort_values(ascending=False)
sns.lineplot(x=range(len(movie_counts)), y=movie_counts.values, ax=axes[0], color='#2E8B57')
axes[0].axhline(y=5, color='red', linestyle='--', label='Corte (5 avalia√ß√µes)')
axes[0].set_yscale('log')
axes[0].set_title('Cauda Longa: Popularidade dos Filmes')
axes[0].set_ylabel('Qtd Avalia√ß√µes (Log)')
axes[0].legend()

# 2. Ratings por Usu√°rio
user_activity = ratings.groupby('userId').size().sort_values(ascending=False)
sns.lineplot(x=range(len(user_activity)), y=user_activity.values, ax=axes[1], color='#4682B4')
axes[1].set_yscale('log')
axes[1].set_title('Atividade dos Usu√°rios')
axes[1].set_ylabel('Qtd Avalia√ß√µes (Log)')

plt.tight_layout()
plt.show()

# 1. WordCloud das Tags
if not tags.empty:
    tag_text = ' '.join(tags['tag'].dropna().astype(str).values)
    wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='viridis').generate(tag_text)
    
    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title('Principais Tags Atribu√≠das pelos Usu√°rios')
    plt.show()

# 2. Distribui√ß√£o de G√™neros
genres_expanded = movies['genres'].str.get_dummies(sep='|')
genre_counts = genres_expanded.sum().sort_values(ascending=False)

plt.figure(figsize=(12, 6))
sns.barplot(x=genre_counts.values, y=genre_counts.index, palette=PALETTE)
plt.title('Quantidade de Filmes por G√™nero')
plt.show()

# 2. Sele√ß√£o Autom√°tica de Colunas Num√©ricas
# Seleciona apenas numeros (float/int)
numeric_df = df_enriched.select_dtypes(include=[np.number])

# 3. Limpeza: Remover colunas que n√£o s√£o features reais (IDs e Timestamps)
# N√£o queremos saber se o 'userId' tem correla√ß√£o com 'rating' (pois √© s√≥ um identificador)
cols_to_exclude = ['userId', 'movieId', 'timestamp']
cols_for_corr = [c for c in numeric_df.columns if c not in cols_to_exclude]

print(f"Calculando correla√ß√£o de {len(cols_for_corr)} features...")
print(f"Colunas inclu√≠das: {cols_for_corr[:5]} ...")

# 4. Calcular a Matriz de Correla√ß√£o
# Isso pode levar alguns segundos dependendo do tamanho do dataset
corr_matrix = numeric_df[cols_for_corr].corr()

# 5. Plotar o Heatmap Gigante
plt.figure(figsize=(24, 20)) # Tamanho bem grande para ler os nomes

sns.heatmap(
    corr_matrix, 
    annot=False,       # False para n√£o poluir visualmente (muitos n√∫meros)
    cmap='RdBu_r',     # Vermelho (Negativo) <-> Azul (Positivo)
    center=0,          # Garante que o branco seja correla√ß√£o zero
    vmin=-1, vmax=1,   # Trava a escala entre -1 e 1
    linewidths=0.1,    # Linhas finas para separar
    cbar_kws={"shrink": 0.8} # Barra de cores menorzinha
)

plt.title('Matriz de Correla√ß√£o Global: Target + M√©tricas + G√™neros + Tags', fontsize=18)
plt.xticks(rotation=90, fontsize=10) # Rotaciona nomes embaixo
plt.yticks(fontsize=10)
plt.tight_layout()
plt.show()

# 6. Extra: Listar as Top Correla√ß√µes com o Rating (O que mais impacta a nota?)
print("\n--- O que mais influencia o 'rating'? (Top 10 Correla√ß√µes) ---")
target_corr = corr_matrix['rating'].drop('rating') # Remove a correla√ß√£o dele com ele mesmo
top_positive = target_corr.sort_values(ascending=False).head(5)
top_negative = target_corr.sort_values(ascending=True).head(5)

print("üìà Top 5 Correla√ß√µes POSITIVAS (Aumentam a nota):")
print(top_positive)
print("\nüìâ Top 5 Correla√ß√µes NEGATIVAS (Diminuem a nota):")
print(top_negative)